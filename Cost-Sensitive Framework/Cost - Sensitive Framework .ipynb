{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Comments of the provided code: \n",
    "    \n",
    "Dataset used:\n",
    "SMS Spam Collection Dataset\n",
    "https://www.kaggle.com/uciml/sms-spam-collection-dataset \n",
    "    \n",
    "    \n",
    "Cost-Sensitive Learning approaches :\n",
    "\n",
    "1.Minimizing the cost (with Bayes-Minimum-Risk-Classifier model and other similar models):\n",
    "\n",
    "        - Without calibration \n",
    "        - With sigmoid calibration \n",
    "        - With isotonic calibration \n",
    "        \n",
    "2.Stratification aka Rebalancing: \n",
    "\n",
    "        -With Under-Sampling \n",
    "        -With Over â€“ Sampling\n",
    "        -With Combination \n",
    "        \n",
    "3.Example Weighting \n",
    "\n",
    "The code is accompanied with a comprehensive report where the results are analyzed and futher insights about the \n",
    "conclusions of the aforementioned approaches are given\n",
    "\n",
    "Happy reading and feel free to contact us for any further details\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from costcla.metrics import cost_loss\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import math\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from costcla.models import BayesMinimumRiskClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timing of the code \n",
    "\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading my data \n",
    "\n",
    "sms = pd.read_csv(\"spam_data.csv\", encoding='latin-1')\n",
    "sms.dropna(how=\"any\", inplace=True, axis=1)\n",
    "sms.columns = ['label', 'message']\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have an imbalace of spam/ham ~= 1/7 \n",
    "sms.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'ham':0, 'spam':1})\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len  \n",
       "0          111  \n",
       "1           29  \n",
       "2          155  \n",
       "3           49  \n",
       "4           61  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check message lenghts \n",
    "sms['message_len'] = sms.message.apply(len)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Message Length')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the imbalance in correlation with message lenght \n",
    "# it seems that spam messages tend to have more characters\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sms[sms.label=='ham'].message_len.plot(bins=35, kind='hist', color='blue', \n",
    "                                       label='Ham messages', alpha=0.6)\n",
    "sms[sms.label=='spam'].message_len.plot(kind='hist', color='red', \n",
    "                                       label='Spam messages', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Message Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4825.0</td>\n",
       "      <td>4825.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>71.023627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>58.016023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>92.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>910.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_num  message_len\n",
       "count     4825.0  4825.000000\n",
       "mean         0.0    71.023627\n",
       "std          0.0    58.016023\n",
       "min          0.0     2.000000\n",
       "25%          0.0    33.000000\n",
       "50%          0.0    52.000000\n",
       "75%          0.0    92.000000\n",
       "max          0.0   910.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean of ham messages = 71 \n",
    "sms[sms.label=='ham'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>747.0</td>\n",
       "      <td>747.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>138.866131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29.183082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>132.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>149.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>224.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label_num  message_len\n",
       "count      747.0   747.000000\n",
       "mean         1.0   138.866131\n",
       "std          0.0    29.183082\n",
       "min          1.0    13.000000\n",
       "25%          1.0   132.500000\n",
       "50%          1.0   149.000000\n",
       "75%          1.0   157.000000\n",
       "max          1.0   224.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mean of spam messages = 138 \n",
    "sms[sms.label=='spam'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenization\n",
    "sms['clean_msg'] = sms.message.apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "      <th>message_len</th>\n",
       "      <th>clean_msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>Ok lar Joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>155</td>\n",
       "      <td>Free entry wkly comp win FA Cup final tkts 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>dun say early hor c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>Nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
       "1   ham                      Ok lar... Joking wif u oni...          0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
       "3   ham  U dun say so early hor... U c already then say...          0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
       "\n",
       "   message_len                                          clean_msg  \n",
       "0          111  Go jurong point crazy Available bugis n great ...  \n",
       "1           29                              Ok lar Joking wif oni  \n",
       "2          155  Free entry wkly comp win FA Cup final tkts 21s...  \n",
       "3           49                    dun say early hor c already say  \n",
       "4           61             Nah think goes usf lives around though  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check our dataset now\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "X = sms.clean_msg\n",
    "y = sms.label_num\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(1393,)\n",
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify = y)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x1500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 25645 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorization \n",
    "\n",
    "# instantiate the vectorizer\n",
    "# vect = CountVectorizer()\n",
    "#tuned vectorizer \n",
    "vect = CountVectorizer(max_features=1500, max_df=0.85, stop_words=stopwords.words(\"english\"))\n",
    "#fit and transform \n",
    "X_train = vect.fit_transform(X_train)\n",
    "# examine the document-term matrix\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x1500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8088 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix\n",
    "X_test = vect.transform(X_test)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x1500 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 25645 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf_idf\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_transformer.fit(X_train)\n",
    "tfidf_transformer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fp, fn, tp, tn\n",
    "fp = np.full((y_test.shape[0],1), 10)\n",
    "fn = np.full((y_test.shape[0],1), 1)\n",
    "tp = np.zeros((y_test.shape[0],1))\n",
    "tn = np.zeros((y_test.shape[0],1))\n",
    "cost_matrix = np.hstack((fp, fn, tp, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Random Forest \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.97      0.85      0.91       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.97      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "78\n",
      "\n",
      "[[1201   28]\n",
      " [   5  159]]\n",
      "\n",
      "\n",
      "Random Forest with example weighting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.98      0.84      0.90       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "60\n",
      "\n",
      "[[1203   30]\n",
      " [   3  157]]\n",
      "\n",
      "\n",
      "RandomForest and Bayes Minimum Risk with no calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.92      1.00      0.96      1206\n",
      "        spam       1.00      0.45      0.62       187\n",
      "\n",
      "    accuracy                           0.93      1393\n",
      "   macro avg       0.96      0.73      0.79      1393\n",
      "weighted avg       0.93      0.93      0.91      1393\n",
      "\n",
      "102\n",
      "\n",
      "[[1206  102]\n",
      " [   0   85]]\n",
      "\n",
      "\n",
      "\n",
      " RandomForest and Bayes Minimum Risk with sigmoid calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1206\n",
      "        spam       1.00      0.73      0.84       187\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.98      0.86      0.91      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "51\n",
      "\n",
      "[[1206   51]\n",
      " [   0  136]]\n",
      "\n",
      "\n",
      "\n",
      " RandomForest and Bayes Minimum Risk with isotonic calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1206\n",
      "        spam       0.99      0.77      0.87       187\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.88      0.92      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n",
      "53\n",
      "\n",
      "[[1205   43]\n",
      " [   1  144]]\n",
      "\n",
      "\n",
      "Random Forest with undersampling\n",
      "Counter({0: 3619, 1: 362})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1206\n",
      "        spam       0.99      0.73      0.84       187\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.98      0.87      0.91      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "[[1205   50]\n",
      " [   1  137]]\n",
      "60\n",
      "\n",
      "\n",
      "\n",
      "Random Forest with oversampling\n",
      "Counter({0: 5600, 1: 560})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (5600) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.98      0.86      0.91       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.93      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1203   27]\n",
      " [   3  160]]\n",
      "57\n",
      "\n",
      "Random Forest with combination\n",
      "Counter({0: 4500, 1: 450})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (4500) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1206\n",
      "        spam       0.99      0.80      0.88       187\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.90      0.93      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n",
      "[[1204   38]\n",
      " [   2  149]]\n",
      "58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Random Forest \")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100, max_depth = 70)\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Random Forest with example weighting\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100, max_depth = 70, class_weight = {0: 1, 1: 10})\n",
    "# clf = RandomForestClassifier(random_state=0, n_estimators=100, max_depth = 70, class_weight = 'balanced')\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T)\n",
    "print('\\n')\n",
    "\n",
    "print(\"RandomForest and Bayes Minimum Risk with no calibration\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100,  max_depth = 70)\n",
    "model = clf.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"\\n RandomForest and Bayes Minimum Risk with sigmoid calibration\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100,  max_depth = 70)\n",
    "cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"\\n RandomForest and Bayes Minimum Risk with isotonic calibration\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100,  max_depth = 70)\n",
    "cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"Random Forest with undersampling\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100,  max_depth = 70)\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 362}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')\n",
    "\n",
    "print(\"Random Forest with oversampling\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100,  max_depth = 70)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 5600, 1: 560}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"Random Forest with combination\")\n",
    "clf = RandomForestClassifier(random_state=0, n_estimators=100,  max_depth = 70)\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 4500, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baeline SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.98      0.88      0.93       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.94      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "62\n",
      "\n",
      "[[1202   22]\n",
      " [   4  165]]\n",
      "\n",
      "\n",
      "SVM with example weighting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.98      0.87      0.92       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.93      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "64\n",
      "\n",
      "[[1202   24]\n",
      " [   4  163]]\n",
      "\n",
      "\n",
      "SVM and Bayes Minimum Risk with no calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1206\n",
      "        spam       1.00      0.79      0.88       187\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.90      0.93      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n",
      "39\n",
      "\n",
      "[[1206   39]\n",
      " [   0  148]]\n",
      "\n",
      "\n",
      "\n",
      " SVM and Bayes Minimum Risk with sigmoid calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1206\n",
      "        spam       1.00      0.71      0.83       187\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.98      0.85      0.90      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "55\n",
      "\n",
      "[[1206   55]\n",
      " [   0  132]]\n",
      "\n",
      "\n",
      "SVM and Bayes Minimum Risk with isotonic calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1206\n",
      "        spam       1.00      0.72      0.84       187\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.98      0.86      0.91      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "52\n",
      "\n",
      "[[1206   52]\n",
      " [   0  135]]\n",
      "\n",
      "\n",
      "SVM with undersampling\n",
      "Counter({0: 3619, 1: 362})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.99      0.85      0.92       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.99      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1205   28]\n",
      " [   1  159]]\n",
      "38\n",
      "\n",
      "\n",
      "\n",
      "SVM with oversampling\n",
      "Counter({0: 5600, 1: 560})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (5600) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.98      0.89      0.93       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.94      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1202   21]\n",
      " [   4  166]]\n",
      "61\n",
      "\n",
      "SVM with combination\n",
      "Counter({0: 4500, 1: 450})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (4500) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.98      0.87      0.92       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.93      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1203   25]\n",
      " [   3  162]]\n",
      "55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Baeline SVM\")\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test,target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) #transpose \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"SVM with example weighting\")\n",
    "# clf = SVC(kernel='linear', C=1, probability=True, class_weight = {0: 1, 1: 10})\n",
    "clf = SVC(kernel='linear', C=1, probability=True, class_weight = {0: 10, 1: 1})\n",
    "# clf = SVC(kernel='linear', C=1, probability=True, class_weight = 'balanced')\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test,target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) #transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"SVM and Bayes Minimum Risk with no calibration\")\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "model = clf.fit(X_train, y_train)\n",
    "prob_train = model.predict_proba(X_train)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "bmr.fit(y_train, prob_train) \n",
    "prob_test = model.predict_proba(X_test)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"\\n SVM and Bayes Minimum Risk with sigmoid calibration\")\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"SVM and Bayes Minimum Risk with isotonic calibration\")\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"SVM with undersampling\")\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 362}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')\n",
    "\n",
    "print(\"SVM with oversampling\")\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 5600, 1: 560}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "\n",
    "print(\"SVM with combination\")\n",
    "clf = SVC(kernel='linear', C=1, probability=True)\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 4500, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Logistic Regression \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.99      0.86      0.92       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.93      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "46\n",
      "\n",
      "[[1204   26]\n",
      " [   2  161]]\n",
      "\n",
      "\n",
      "Logistic Regression with example weighting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       1.00      0.86      0.92       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.99      0.93      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "27\n",
      "\n",
      "[[1206   27]\n",
      " [   0  160]]\n",
      "\n",
      "\n",
      "Logistic Regression and Bayes Minimum Risk with no calibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.93      1.00      0.96      1206\n",
      "        spam       1.00      0.50      0.66       187\n",
      "\n",
      "    accuracy                           0.93      1393\n",
      "   macro avg       0.96      0.75      0.81      1393\n",
      "weighted avg       0.94      0.93      0.92      1393\n",
      "\n",
      "94\n",
      "\n",
      "[[1206   94]\n",
      " [   0   93]]\n",
      "\n",
      "\n",
      "\n",
      " Logistic Regression and Bayes Minimum Risk with sigmoid calibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.97      1206\n",
      "        spam       1.00      0.66      0.79       187\n",
      "\n",
      "    accuracy                           0.95      1393\n",
      "   macro avg       0.97      0.83      0.88      1393\n",
      "weighted avg       0.96      0.95      0.95      1393\n",
      "\n",
      "64\n",
      "\n",
      "[[1206   64]\n",
      " [   0  123]]\n",
      "\n",
      "\n",
      "Logistic Regression and Bayes Minimum Risk with isotonic calibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.96      1.00      0.98      1206\n",
      "        spam       1.00      0.70      0.82       187\n",
      "\n",
      "    accuracy                           0.96      1393\n",
      "   macro avg       0.98      0.85      0.90      1393\n",
      "weighted avg       0.96      0.96      0.96      1393\n",
      "\n",
      "56\n",
      "\n",
      "[[1206   56]\n",
      " [   0  131]]\n",
      "\n",
      "\n",
      "Logistic Regression with undersampling\n",
      "Counter({0: 3619, 1: 362})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (5600) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      1206\n",
      "        spam       0.99      0.79      0.88       187\n",
      "\n",
      "    accuracy                           0.97      1393\n",
      "   macro avg       0.98      0.90      0.93      1393\n",
      "weighted avg       0.97      0.97      0.97      1393\n",
      "\n",
      "[[1205   39]\n",
      " [   1  148]]\n",
      "49\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression with oversampling\n",
      "Counter({0: 5600, 1: 560})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (4500) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.99      0.84      0.91       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1205   30]\n",
      " [   1  157]]\n",
      "40\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression with combination\n",
      "Counter({0: 4500, 1: 450})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99      1206\n",
      "        spam       1.00      0.82      0.90       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.99      0.91      0.94      1393\n",
      "weighted avg       0.98      0.98      0.97      1393\n",
      "\n",
      "[[1206   34]\n",
      " [   0  153]]\n",
      "34\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Logistic Regression \")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1) \n",
    "# clf = LogisticRegression()\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test,target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"Logistic Regression with example weighting\")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1, class_weight = {0:10, 1:1}) \n",
    "# clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1, class_weight = 'balanced')\n",
    "clf = LogisticRegression()\n",
    "model = clf.fit(X_train, y_train)\n",
    "pred_test = model.predict(X_test)\n",
    "print(classification_report(y_test, pred_test,target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"Logistic Regression and Bayes Minimum Risk with no calibration\")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1) \n",
    "# clf = LogisticRegression()\n",
    "model = clf.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "# print(prob_test)\n",
    "#A example-dependent cost-sensitive binary Bayes minimum risk classifier.\n",
    "# http://albahnsen.github.io/CostSensitiveClassification/BayesMinimumRiskClassifier.html \n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "# print(pred_test)\n",
    "print(classification_report(y_test, pred_test,target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"\\n Logistic Regression and Bayes Minimum Risk with sigmoid calibration\")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1)\n",
    "# clf = LogisticRegression()\n",
    "cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test,target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"Logistic Regression and Bayes Minimum Risk with isotonic calibration\")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1)\n",
    "# clf = LogisticRegression()\n",
    "cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"Logistic Regression with undersampling\")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1)\n",
    "# clf = LogisticRegression()\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 362}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"Logistic Regression with oversampling\")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1)\n",
    "# clf = LogisticRegression()\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 5600, 1: 560}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"Logistic Regression with combination\")\n",
    "clf = LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 40, l1_ratio=1)\n",
    "# clf = LogisticRegression()\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 4500, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MultinomialNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1206\n",
      "        spam       0.96      0.90      0.93       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.97      0.95      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1199   18]\n",
      " [   7  169]]\n",
      "88\n",
      "\n",
      "\n",
      "\n",
      "MultinomialNB with example weighting\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       1.00      0.84      0.91       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.99      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "30\n",
      "\n",
      "[[1206   30]\n",
      " [   0  157]]\n",
      "\n",
      "\n",
      "MultinomialNB and Bayes Minimum Risk with no calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.99      0.85      0.91       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "48\n",
      "\n",
      "[[1204   28]\n",
      " [   2  159]]\n",
      "\n",
      "\n",
      "\n",
      " MultinomialNB and Bayes Minimum Risk with sigmoid calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.99      0.84      0.91       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.92      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "40\n",
      "\n",
      "[[1205   30]\n",
      " [   1  157]]\n",
      "\n",
      "\n",
      "MultinomialNB and Bayes Minimum Risk with isotonic calibration\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99      1206\n",
      "        spam       1.00      0.83      0.91       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.99      0.91      0.95      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "32\n",
      "\n",
      "[[1206   32]\n",
      " [   0  155]]\n",
      "\n",
      "\n",
      "MultinomialNB with undersampling\n",
      "Counter({0: 3619, 1: 362})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.98      0.89      0.93       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.94      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1202   21]\n",
      " [   4  166]]\n",
      "61\n",
      "\n",
      "\n",
      "\n",
      "MultinomialNB with oversampling\n",
      "Counter({0: 5600, 1: 560})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1206\n",
      "        spam       0.97      0.91      0.94       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.98      0.96      0.97      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1201   16]\n",
      " [   5  171]]\n",
      "66\n",
      "\n",
      "\n",
      "\n",
      "MultinomialNB with combination\n",
      "Counter({0: 4500, 1: 450})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99      1206\n",
      "        spam       0.97      0.90      0.93       187\n",
      "\n",
      "    accuracy                           0.98      1393\n",
      "   macro avg       0.97      0.95      0.96      1393\n",
      "weighted avg       0.98      0.98      0.98      1393\n",
      "\n",
      "[[1200   19]\n",
      " [   6  168]]\n",
      "79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (5600) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\nickg\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py:257: UserWarning: After over-sampling, the number of samples (4500) in class 0 will be larger than the number of samples in the majority class (class #0 -> 3619)\n",
      "  n_samples_majority))\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline MultinomialNB\")\n",
    "clf = MultinomialNB()\n",
    "model = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')\n",
    "\n",
    "print(\"MultinomialNB with example weighting\")\n",
    "clf = MultinomialNB(class_prior = [10,1])\n",
    "model = clf.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "print(\"MultinomialNB and Bayes Minimum Risk with no calibration\")\n",
    "clf = MultinomialNB()\n",
    "model = clf.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"\\n MultinomialNB and Bayes Minimum Risk with sigmoid calibration\")\n",
    "clf = MultinomialNB()\n",
    "cc = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print(\"MultinomialNB and Bayes Minimum Risk with isotonic calibration\")\n",
    "clf = MultinomialNB()\n",
    "cc = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3)\n",
    "model = cc.fit(X_train, y_train)\n",
    "prob_test = model.predict_proba(X_test)\n",
    "bmr = BayesMinimumRiskClassifier(calibration=False)\n",
    "pred_test = bmr.predict(prob_test, cost_matrix)\n",
    "print(classification_report(y_test, pred_test, target_names = ['ham', 'spam']))\n",
    "loss = cost_loss(y_test, pred_test, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print(confusion_matrix(y_test, pred_test).T) # transpose \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "print(\"MultinomialNB with undersampling\")\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 362}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')\n",
    "\n",
    "print(\"MultinomialNB with oversampling\")\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 5600, 1: 560}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)\n",
    "print('\\n')\n",
    "\n",
    "print(\"MultinomialNB with combination\")\n",
    "sampler = RandomUnderSampler(sampling_strategy={0: 3619, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_train, y_train)\n",
    "sampler = RandomOverSampler(sampling_strategy={0: 4500, 1: 450}, random_state=0)\n",
    "X_rs, y_rs = sampler.fit_resample(X_rs, y_rs)\n",
    "print(Counter(y_rs))\n",
    "model = clf.fit(X_rs, y_rs)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n",
    "print(confusion_matrix(y_test, y_pred).T) # transpose to align with slides\n",
    "loss = cost_loss(y_test, y_pred, cost_matrix)\n",
    "print(\"%d\\n\" %loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 16.694284677505493 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
